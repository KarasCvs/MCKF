\documentclass[dvipdfmx]{jsarticle}
\setlength{\textheight}{\paperheight}     % ひとまず紙面を本文領域に
\setlength{\topmargin}{-5.4truemm}         % 上の余白を20mm(=1inch-5.4mm)に
\setlength{\parindent}{1em}                % 缩进1字符
\addtolength{\topmargin}{-\headheight}
\addtolength{\topmargin}{-\headsep}       % ヘッダの分だけ本文領域を移動させる
\addtolength{\textheight}{-40truemm}       % 下の余白も20mmに幅の設定
\setlength{\textwidth}{\paperwidth}       % ひとまず紙面を本文領域に
\setlength{\oddsidemargin}{-5.4truemm}     % 左の余白を20mm(=1inch-5.4mm)に
\setlength{\evensidemargin}{-5.4truemm}
\addtolength{\textwidth}{-40truemm}        % 右の余白も20mmに
\usepackage{fancybox,ascmac}                % 文字を囲むため
\usepackage{booktabs}                       % 表格加粗线
\usepackage{diagbox}                        % 表格斜线
\usepackage[dvipdfmx]{graphicx}             % 図を挿入するため
\usepackage{float}                          %设置图片浮动位置的宏包
\usepackage{subfigure}                      %插入多图时用子图显示的宏包
\usepackage{caption}                        % 图片标题
\usepackage{amsfonts}	                      % \mathbb{R}を使うため
\usepackage{bm}                             % ベクトルの太字用
\usepackage{amsmath}                        % \begin{align}を使うため \mathcal{X}$导入花体字母
\usepackage{indentfirst}                    % 首行缩进 \noindent不缩进 \indent主动缩进 \hangindent 2em \hangafter=0 整段缩进
\begin{document}

\title{Maximum correntropy unscented filter}           % 文章名の指定
\maketitle                                 % 文書・著者名・日付の出力
State space function.
\begin{equation}\label{Func.}\begin{aligned}
    \mathbf{x}(k)=\mathrm{f}(k-1, \mathbf{x}(k-1))+\mathbf{q}(k-1) \\
    \mathbf{y}(k)=\mathrm{h}(k, \mathbf{x}(k))+\mathbf{r}(k)
\end{aligned}\end{equation}
Expectation of kernel.
\begin{equation}\label{Func.}\begin{aligned}
    V(X, Y)=\mathrm{E}[\kappa(X, Y)]=\int \kappa(x, y) \mathrm{d} \mathrm{F}_{X Y}(x, y)
\end{aligned}\end{equation}
Kernel function.
\begin{equation}\label{Func.}\begin{aligned}
    \kappa(x, y)=\mathrm{G}_{\sigma}(e)=\exp \left(-\frac{e^{2}}{2 \sigma^{2}}\right)
\end{aligned}\end{equation}
Sigma points.
\begin{equation}\label{Func.}\begin{aligned}
    \begin{array}{l}
        \chi^{0}(k-1 \mid k-1)=\widehat{\mathbf{x}}(k-1 \mid k-1) \\
        \chi^{i}(k-1 \mid k-1)=\widehat{\mathbf{x}}(k-1 \mid k-1) \\
        \quad+(\sqrt{(n+\lambda) \mathbf{P}(k-1 \mid k-1)})_{i}, \text { for } i=1 \ldots n \\
        \chi^{i}(k-1 \mid k-1)=\widehat{\mathbf{x}}(k-1 \mid k-1) \\
        -(\sqrt{(n+\lambda) \mathbf{P}(k-1 \mid k-1)})_{i-n}, \text { for } i=n+1 \ldots 2 n .(9)
        \end{array}
\end{aligned}\end{equation}
Parameter of UT.
\begin{equation}\label{Func.}\begin{aligned}
    \lambda=\alpha^{2}(n+\phi)-n
\end{aligned}\end{equation}
Sigma points after UT.
\begin{equation}\label{Func.}\begin{aligned}
    \chi^{i *}(k \mid k-1)=\mathrm{f}\left(k-1, \chi^{i}(k-1 \mid k-1)\right), \text { for } i=0 \ldots 2 n
\end{aligned}\end{equation}
Mean of UTed sigma points, which will be the priori of state.
\begin{equation}\label{Func.}\begin{aligned}
    \widehat{\mathbf{x}}(k \mid k-1)=\sum_{i=0}^{2 n} w_{m}^{i} \chi^{i *}(k \mid k-1) \\
\end{aligned}\end{equation}
Covariance matrix of $\chi^{i *}$and$\widehat{\mathbf{x}}(k \mid k-1)$, which should be the
Predicted estimate covariance of KF.
\begin{equation}\label{Func.}\begin{aligned}
    \begin{array}{c}
        \mathbf{P}(k \mid k-1)=\sum_{i=0}^{2 n} w_{c}^{i}\left[\chi^{i *}(k \mid k-1)-\widehat{\mathbf{x}}(k \mid k-1)\right] \\
        \times\left[\chi^{i *}(k \mid k-1)-\widehat{\mathbf{x}}(k \mid k-1)\right]^{T}+\mathbf{Q}(k-1)
        \end{array}
\end{aligned}\end{equation}
UTed observation.
\begin{equation}\label{Func.}\begin{aligned}
    \gamma^{i}(k)=\mathrm{h}\left(k, \chi^{i}(k \mid k-1)\right), \text { for } i=0 \ldots 2 n
\end{aligned}\end{equation}
With weight, like a mean of UTed observation.
\begin{equation}\label{Func.}\begin{aligned}
    \widehat{\mathbf{y}}(k)=\sum_{i=0}^{2 n} w_{m}^{i} \gamma^{i}(k)
\end{aligned}\end{equation}
Covariance matrix of$\chi^{i *}$and$\gamma^{i}$, this shows the relationship between $\chi^{i *}$and$\gamma^{i}$.
\begin{equation}\label{Func.}\begin{aligned}
    \mathbf{P}_{\mathbf{x y}}(k)=\sum_{i=0}^{2 n} w_{c}^{i}\left[\chi^{i}(k \mid k-1)-\widehat{\mathbf{x}}(k \mid k-1)\right]\left[\gamma^{i}(k)-\widehat{\mathbf{y}}(k)\right]^{T}
\end{aligned}\end{equation}
$x(k)$is the real state, means $\eta$is observation error.
\begin{equation}\label{Func.}\begin{aligned}
    \eta(\mathbf{x}(k))=\mathbf{x}(k)-\widehat{\mathbf{x}}(k \mid k-1)
\end{aligned}\end{equation}
The measurement slope matrix, I don't know what is this.
\begin{equation}\label{Func.}\begin{aligned}
    \mathbf{H}(k)=\left(\mathbf{P}^{-1}(k \mid k-1) \mathbf{P}_{\mathbf{x y}}(k)\right)^{T}
\end{aligned}\end{equation}
Because of Wang et al. (2010):
\begin{equation}\label{Func.}\begin{aligned}
    \mathbf{y}(k) \approx \widehat{\mathbf{y}}(k)+\mathbf{H}(k)(\mathbf{x}(k)-\widehat{\mathbf{x}}(k \mid k-1))+\mathbf{r}(k)
\end{aligned}\end{equation}
For (7), (10), (14), the statistical linear regression model will be
\begin{equation}\label{Func.}\begin{aligned}
    \left[\begin{array}{c}
        \widehat{\mathbf{x}}(k \mid k-1) \\
        \mathbf{y}(k)-\widehat{\mathbf{y}}(k)+\mathbf{H}(k) \widehat{\mathbf{x}}(k \mid k-1)
        \end{array}\right]=\left[\begin{array}{c}
        \mathbf{I} \\
        \mathbf{H}(k)
        \end{array}\right] \mathbf{x}(k)+\xi(k)
\end{aligned}\end{equation}
Where
\begin{equation}\label{Func.}\nonumber\begin{aligned}
    \xi(k)=\left[\begin{array}{c}
        \eta(\mathbf{x}(k)) \\
        \mathbf{r}(k)
        \end{array}\right]
\end{aligned}\end{equation}
With
\begin{equation}\label{Func.}\begin{aligned}
    \begin{aligned}
        \Xi(k) &=\mathrm{E}\left[\xi(k) \xi^{T}(k)\right] \\
        &=\left[\begin{array}{cc}
        \mathbf{P}(k \mid k-1) & 0 \\
        0 & \mathbf{R}(k)
        \end{array}\right] \\
        &=\left[\begin{array}{cc}
        \mathbf{S}_{p}(k \mid k-1) \mathbf{S}_{p}^{T}(k \mid k-1) & 0 \\
        0 & \mathbf{S}_{r}(k) \mathbf{S}_{r}^{T}(k)
        \end{array}\right] \\
        &=\mathbf{S}(k) \mathbf{S}^{T}(k)
        \end{aligned}
\end{aligned}\end{equation}
Transform (15) to
\begin{equation}\label{Func.}\begin{aligned}
    \mathbf{D}(k)=\mathbf{W}(k) \mathbf{x}(k)+\mathbf{e}(k)
\end{aligned}\end{equation}
Where
\begin{equation}\nonumber\label{Func.}\begin{aligned}
    \begin{aligned}
        \mathbf{D}(k) &=\mathbf{S}^{-1}(k)\left[\begin{array}{c}
        \widehat{\mathbf{x}}(k \mid k-1) \\
        \mathbf{y}(k)-\widehat{\mathbf{y}}(k)+\mathbf{H}(k) \widehat{\mathbf{x}}(k \mid k-1)
        \end{array}\right] \\
        \mathbf{W}(k) &=\mathbf{S}^{-1}(k)\left[\begin{array}{c}
        \mathbf{I} \\
        \mathbf{H}(k)
        \end{array}\right] \\
        \mathbf{e}(k) &=\mathbf{S}^{-1}(k) \xi(k)
        \end{aligned}
\end{aligned}\end{equation}
With $\mathrm{E}\left[\mathbf{e}(k) \mathbf{e}^{T}(k)\right]=\mathbf{I}$.
Define a cost function based on the MCC:
\begin{equation}\label{Func.}\begin{aligned}
    J_{L}(\mathbf{x}(k))=\sum_{i=1}^{L} \mathrm{G}_{\sigma}\left(d_{i}(k)-\mathbf{w}_{i}(k) \mathbf{x}(k)\right)
\end{aligned}\end{equation}
The optimal solution is:
\begin{equation}\label{Func.}\begin{aligned}
    \frac{\partial J_{L}(\mathbf{x}(k))}{\partial \mathbf{x}(k)}=0
\end{aligned}\end{equation}
Which should able to be transformed to:
\begin{equation}\label{Func.}\begin{aligned}
    \begin{aligned}
        \mathbf{x}(k)=&\left(\sum_{i=1}^{L}\left(\mathrm{G}_{\sigma}\left(e_{i}(k)\right) \mathbf{w}_{i}^{T}(k) \mathbf{w}_{i}(k)\right)\right)^{-1} \\
        & \times\left(\sum_{i=1}^{L}\left(\mathrm{G}_{\sigma}\left(e_{i}(k)\right) \mathbf{w}_{i}^{T}(k) d_{i}(k)\right)\right)
        \end{aligned}
\end{aligned}\end{equation}
But I'm not sure how.
Since $e_{i}(k)=d_{i}(k)-\mathbf{w}_{i}(k) \mathbf{x}(k)$, obtain x(k) with
a fixed-point iterative algorithm
\begin{equation}\label{Func.}\begin{aligned}
    \widehat{\mathbf{x}}(k)_{t+1}=g\left(\widehat{\mathbf{x}}(k)_{t}\right)
\end{aligned}\end{equation}
For x(k) is a matrix, (20) can also be expressed as
\begin{equation}\label{Func.}\begin{aligned}
    \mathbf{x}(k)=\left(\mathbf{W}^{T}(k) \mathbf{C}(k) \mathbf{W}(k)\right)^{-1} \mathbf{W}^{T}(k) \mathbf{C}(k) \mathbf{D}(k)
\end{aligned}\end{equation}
\begin{equation}\label{Func.}\begin{aligned}
    \begin{array}{l}
        \text { where } \mathbf{C}(k)=\left[\begin{array}{cc}
        \mathbf{C}_{x}(k) & 0 \\
        0 & \mathbf{C}_{y}(k)
        \end{array}\right], \text { with } \\
        \mathbf{C}_{x}(k)=\operatorname{diag}\left(\mathbf{G}_{\sigma}\left(e_{1}(k)\right), \ldots, \mathbf{G}_{\sigma}\left(e_{n}(k)\right)\right) \\
        \mathbf{C}_{y}(k)=\operatorname{diag}\left(\mathbf{G}_{\sigma}\left(e_{n+1}(k)\right), \ldots, \mathbf{G}_{\sigma}\left(e_{n+m}(k)\right)\right)
        \end{array}
\end{aligned}\end{equation}
So the correct function of KF is
\begin{equation}\label{Func.}\begin{aligned}
    \mathbf{x}(k)=\widehat{\mathbf{x}}(k \mid k-1)+\overline{\mathbf{K}}(k)(\mathbf{y}(k)-\widehat{\mathbf{y}}(k))
\end{aligned}\end{equation}
where
\begin{equation}\label{Func.}\begin{aligned}
    \left\{\begin{array}{l}
        \overline{\mathbf{K}}(k)=\overline{\mathbf{P}}(k \mid k-1) \mathbf{H}^{T}(k)\left(\mathbf{H}(k) \overline{\mathbf{P}}(k \mid k-1) \mathbf{H}^{T}(k)+\overline{\mathbf{R}}(k)\right)^{-1} \\
        \overline{\mathbf{P}}(k \mid k-1)=\mathbf{S}_{p}(k \mid k-1) \mathbf{C}_{x}^{-1}(k) \mathbf{S}_{p}^{T}(k \mid k-1) \\
        \overline{\mathbf{R}}(k)=\mathbf{S}_{r}(k) \mathbf{C}_{y}^{-1}(k) \mathbf{S}_{r}^{T}(k)
        \end{array}\right.
\end{aligned}\end{equation}
And the corresponding covariance matrix is updated by
\begin{equation}\label{Func.}\begin{aligned}
        \mathbf{P}(k \mid k)=&(\mathbf{I}-\overline{\mathbf{K}}(k) \mathbf{H}(k)) \mathbf{P}(k \mid k-1)(\mathbf{I}-\overline{\mathbf{K}}(k) \mathbf{H}(k))^{T} \\
        &+\overline{\mathbf{K}}(k) \mathbf{R}(k) \overline{\mathbf{K}}^{T}(k)
\end{aligned}\end{equation}
\end{document}